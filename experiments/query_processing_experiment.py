"""
Query Processing Experiment

This script runs experiments to compare different query processing techniques:
- Original queries (baseline)
- Simple query expansion using WordNet
- LLM-based query expansion
- HyDE (Hypothetical Document Embeddings)

It evaluates how these techniques affect retrieval performance.
"""

import os
import sys
import pickle
import time
from tqdm import tqdm
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Callable

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.components.rag_components import DocumentChunker, EmbeddingProvider, RetrievalMethods, QueryProcessor
from src.components.evaluation import RAGEvaluator
from src.utils.experiment_tracker import ExperimentTracker
from src.components.enhanced_hyde import EnhancedHyDE

# Mock LLM function for HyDE and other experiments that need an LLM
def mock_llm_function(prompt: str) -> str:
    """
    Mock LLM function that generates text based on a prompt
    In a real system, this would call an API or run a local model
    
    Args:
        prompt: The prompt to generate from
        
    Returns:
        Generated text
    """
    # For demo, generate some text that looks like it might be from an LLM
    from transformers import pipeline
    
    try:
        # Try to load a small local model for text generation
        model = pipeline(
            "text-generation",
            model="distilgpt2",
            max_new_tokens=100,
            pad_token_id=50256
        )
        
        response = model(prompt)[0]['generated_text']
        
        # Remove the prompt from the response
        response = response[len(prompt):].strip()
        
        return response
    except:
        # Fallback to a simple template-based response if model loading fails
        if "Write a short but detailed passage" in prompt:
            # This is a HyDE request
            query = prompt.split("'")[1] if "'" in prompt else prompt.split(":")[1]
            return f"Here is information about {query}. This is a detailed passage that contains relevant facts and information that would be helpful to answer the question."
            
        return "This is a mock response from the LLM. In a real system, this would be a detailed, informative response generated by a language model."


# --- run_query_processing_experiment Function (WITH TYPE CONVERSION FIX) ---
def run_query_processing_experiment(
    corpus_file: str = "data/sample_corpus.pkl",
    queries_file: str = "data/sample_queries.pkl",
    sample_size: int = 50,
    num_queries: int = 5,
    chunk_strategy: str = "fixed",
    chunk_size: int = 128,      # Default is int
    chunk_overlap: int = 0,       # Default is int
    embedding_model: str = "all-MiniLM-L6-v2",
    llm_function: Callable = mock_llm_function,
    output_dir: str = "results"
):
    """
    Run experiments to compare different query processing techniques
    """
    # --- Keep os.makedirs, data loading, sampling exactly as in paste-2.txt ---
    os.makedirs(output_dir, exist_ok=True)
    try:
        with open(corpus_file, "rb") as f: corpus_data = pickle.load(f)
    except FileNotFoundError: print(f"Corpus file not found: {corpus_file}"); return None
    try:
        with open(queries_file, "rb") as f: query_data = pickle.load(f)
    except FileNotFoundError: print(f"Queries file not found: {queries_file}"); return None
    if sample_size < len(corpus_data): corpus_sample = corpus_data[:sample_size]
    else: corpus_sample = corpus_data
    if num_queries < len(query_data): queries_sample = query_data[:num_queries]
    else: queries_sample = query_data
    print(f"Running query processing experiments with {len(corpus_sample)} documents and {len(queries_sample)} queries")

    # --- Keep tracker initialization exactly as in paste-2.txt ---
    tracker = ExperimentTracker("query_processing_experiment")

    # --- Load best configuration (Keep exactly as in paste-2.txt) ---
    best_config = load_best_configuration(output_dir)

    # --- Assign and FIX TYPES for chunk_size and chunk_overlap ---
    # Start with function defaults (which are ints)
    current_chunk_strategy = chunk_strategy
    current_chunk_size = chunk_size
    current_chunk_overlap = chunk_overlap
    current_embedding_model = embedding_model

    # Overwrite with loaded config if available
    if best_config:
        current_chunk_strategy = best_config.get("chunking_strategy", current_chunk_strategy)
        # Get potential string/object values from loaded config
        loaded_chunk_size = best_config.get("chunk_size")
        loaded_chunk_overlap = best_config.get("chunk_overlap")
        current_embedding_model = best_config.get("embedding_model", current_embedding_model)

        # --- ADDED TYPE CONVERSION WITH ERROR HANDLING ---
        if loaded_chunk_size is not None:
            try:
                current_chunk_size = int(loaded_chunk_size) # Convert to int
            except (ValueError, TypeError):
                print(f"Warning: Could not convert loaded chunk_size '{loaded_chunk_size}' to int. Using default: {chunk_size}")
                current_chunk_size = chunk_size # Fallback to default int
        # No else needed, current_chunk_size already holds the default if loaded_chunk_size is None

        if loaded_chunk_overlap is not None:
            try:
                current_chunk_overlap = int(loaded_chunk_overlap) # Convert to int
            except (ValueError, TypeError):
                print(f"Warning: Could not convert loaded chunk_overlap '{loaded_chunk_overlap}' to int. Using default: {chunk_overlap}")
                current_chunk_overlap = chunk_overlap # Fallback to default int
        # No else needed, current_chunk_overlap already holds the default if loaded_chunk_overlap is None
        # --- END ADDED TYPE CONVERSION ---

    # Use the correctly typed variables from now on
    # Renaming for clarity, using values determined above
    final_chunk_strategy = current_chunk_strategy
    final_chunk_size = current_chunk_size # This is now guaranteed to be an int
    final_chunk_overlap = current_chunk_overlap # This is now guaranteed to be an int
    final_embedding_model = current_embedding_model

    # Print the final values being used (and their types)
    print(f"Using chunking strategy: {final_chunk_strategy}")
    print(f"Using chunk size: {final_chunk_size} (type: {type(final_chunk_size).__name__})") # Should be int
    print(f"Using chunk overlap: {final_chunk_overlap} (type: {type(final_chunk_overlap).__name__})") # Should be int
    print(f"Using embedding model: {final_embedding_model}")

    # Log experiment configuration (use final values)
    tracker.log_experiment_config({
        "dataset": os.path.basename(corpus_file),
        "sample_size": len(corpus_sample),
        "num_queries": len(queries_sample),
        "chunk_strategy": final_chunk_strategy,
        "chunk_size": final_chunk_size,         # Log the int value
        "chunk_overlap": final_chunk_overlap,     # Log the int value
        "embedding_model": final_embedding_model
    })

    # Apply chunking strategy (use final, correctly typed values)
    # --- This call should now receive integers ---
    try:
        if final_chunk_strategy == "fixed":
            chunked_docs = DocumentChunker.chunk_by_fixed_size(
                corpus_sample, chunk_size=final_chunk_size, overlap=final_chunk_overlap
            )
        elif final_chunk_strategy == "paragraph":
            chunked_docs = DocumentChunker.chunk_by_paragraph(corpus_sample)
        elif final_chunk_strategy == "semantic":
            chunked_docs = DocumentChunker.chunk_by_semantic_units(corpus_sample)
        else:
            raise ValueError(f"Unknown chunking strategy: {final_chunk_strategy}")
    except TypeError as te:
         # Extra safety check in case conversion failed unexpectedly
         print(f"\nCRITICAL ERROR during chunking: {te}")
         print(f"Values passed: size={final_chunk_size} (type {type(final_chunk_size).__name__}), overlap={final_chunk_overlap} (type {type(final_chunk_overlap).__name__})")
         raise te
    except Exception as e:
         print(f"Error during chunking: {e}"); return None

    print(f"Created {len(chunked_docs)} chunks")

    # --- Extract texts and IDs (Keep exactly as in paste-2.txt, assumes chunking works) ---
    # NOTE: Consider adding validation loop here if chunking methods might not return dicts with 'text'/'chunk_id'
    try:
        chunk_texts = [doc["text"] for doc in chunked_docs]
        doc_ids = [doc["chunk_id"] for doc in chunked_docs]
    except KeyError as ke:
        print(f"ERROR: KeyError '{ke}' accessing chunk data. Ensure chunking strategy '{final_chunk_strategy}' returns dicts with 'text' and 'chunk_id'.")
        return None

    # --- Generate embeddings (Keep exactly as in paste-2.txt) ---
    print(f"Generating embeddings using {final_embedding_model}...")
    try:
        # Requires ORIGINAL static EmbeddingProvider
        chunk_embeddings = EmbeddingProvider.get_sentence_transformer_embeddings(
            chunk_texts, model_name=final_embedding_model
        )
    except AttributeError as ae:
        print(f"\nERROR: {ae}. Ensure EmbeddingProvider class uses ORIGINAL static methods.")
        return None
    except Exception as e:
        print(f"Error generating embeddings: {e}"); return None

    # --- Define query techniques (Keep exactly as in paste-2.txt) ---
    query_techniques = [
        {"name": "original", "description": "Original query without modification"},
        {"name": "simple_expansion", "description": "Expand with synonyms", "method": "simple"},
        {"name": "llm_expansion", "description": "Multiple queries from LLM", "method": "llm"},
        {"name": "hyde", "description": "Hypothetical Document Embeddings"},
        {"name": "enhanced_hyde", "description": "Enhanced Hypothetical Document Embeddings"},
        {"name": "multi_hyde", "description": "Multi-document HyDE"}, # Note: evaluate function missing for this
        {"name": "hybrid_hyde", "description": "Hybrid query-HyDE approach"} # Note: evaluate function missing for this
    ]

    # --- Test each query technique (Keep loop structure exactly as in paste-2.txt) ---
    for technique in query_techniques:
        technique_name = technique["name"]
        print(f"\nEvaluating query technique: {technique_name}")
        try:
            start_time = time.time()
            result_metrics = {} # Initialize

            # --- Call appropriate evaluation function based on name ---
            # --- KEEP THIS if/elif block EXACTLY as in paste-2.txt ---
            if technique_name == "original":
                result_metrics = evaluate_original_queries(queries_sample, chunk_embeddings, chunked_docs, doc_ids, final_embedding_model)
            elif technique_name == "simple_expansion" or technique_name == "llm_expansion":
                result_metrics = evaluate_expanded_queries(queries_sample, chunk_embeddings, chunked_docs, doc_ids, final_embedding_model, technique["method"], llm_function)
            elif technique_name == "hyde":
                result_metrics = evaluate_hyde(queries_sample, chunk_embeddings, chunked_docs, doc_ids, final_embedding_model, llm_function)
            elif technique_name == "enhanced_hyde":
                result_metrics = evaluate_enhanced_hyde(queries_sample, chunk_embeddings, chunked_docs, doc_ids, final_embedding_model, llm_function)
            # --- MISSING EVAL FUNCTIONS FOR: multi_hyde, hybrid_hyde ---
            # --- If you run these, you will get an error later ---
            else:
                 print(f"Warning: Evaluation function for technique '{technique_name}' not implemented in the main loop. Skipping evaluation.")
                 continue # Skip to next technique if no eval function is called

            technique_time = time.time() - start_time

            # Log results (Keep logging logic exactly as in paste-2.txt, check keys)
            # Define sanitize helper if not global
            def sanitize_dict_for_json(d):
                if not isinstance(d, dict): return d
                return {k: list(v) if isinstance(v, set) else v for k, v in d.items()}

            log_data = {
                "technique": technique_name,
                "description": technique["description"],
                "processing_time_sec": technique_time,
                # Use .get for safety in case metrics are missing
                "queries_per_second": len(queries_sample) / technique_time if technique_time > 0 else 0,
                **sanitize_dict_for_json({f"metric_{k}": v for k, v in result_metrics.items()})
            }
            tracker.log_iteration(log_data)

        except Exception as e:
            print(f"Error evaluating technique {technique_name}: {e}")
            import traceback; traceback.print_exc()
            # Log failure
            failure_log_data = {"technique": technique_name, "status": "Failed", "error": str(e)}
            tracker.log_iteration(failure_log_data)
            continue # Continue to next technique
    # --- End technique loop ---

    # --- Generate report (Keep exactly as in paste-2.txt) ---
    print("Generating experiment report...")
    try:
        report_path = tracker.generate_report()
        print(f"Report generated at {report_path}")
        return report_path
    except Exception as e:
        print(f"Error generating report: {e}"); return None

def evaluate_original_queries(
    queries: List[Dict[str, Any]],
    chunk_embeddings: np.ndarray,
    chunked_docs: List[Dict[str, str]],
    doc_ids: List[str],
    embedding_model: str
) -> Dict[str, float]:
    """Evaluate retrieval with original queries"""
    # Initialize metrics
    precision_at_1_sum = 0
    precision_at_3_sum = 0
    precision_at_5_sum = 0
    recall_at_3_sum = 0
    recall_at_5_sum = 0
    mrr_sum = 0
    
    # Process each query
    for query_obj in tqdm(queries, desc="Original queries"):
        query = query_obj["question"]
        expected_answer = query_obj.get("answer", "")
        
        # Find relevant chunks (simplified approach)
        relevant_chunks = []
        
        query_words = set(query.lower().split())
        answer_words = set(expected_answer.lower().split())
        combined_words = query_words.union(answer_words)
        
        for i, doc_id in enumerate(doc_ids):
            # Get document text
            doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
            if doc_idx is None:
                continue
                
            doc_text = chunked_docs[doc_idx]["text"]
            doc_words = set(doc_text.lower().split())
            
            # Calculate word overlap
            overlap = len(combined_words.intersection(doc_words))
            if overlap >= min(3, len(combined_words)):
                relevant_chunks.append(doc_id)
        
        # If no relevant chunks found, use chunks with highest word overlap
        if not relevant_chunks:
            overlaps = []
            for i, doc_id in enumerate(doc_ids):
                doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
                if doc_idx is None:
                    continue
                    
                doc_text = chunked_docs[doc_idx]["text"]
                doc_words = set(doc_text.lower().split())
                overlap = len(combined_words.intersection(doc_words))
                overlaps.append((doc_id, overlap))
            
            # Sort by overlap and take top 2
            overlaps.sort(key=lambda x: x[1], reverse=True)
            relevant_chunks = [doc_id for doc_id, _ in overlaps[:2]]
        
        # Embed and search
        query_embedding = EmbeddingProvider.get_sentence_transformer_embeddings(
            [query], model_name=embedding_model
        )[0]
        
        search_results = RetrievalMethods.vector_search(
            query_embedding, chunk_embeddings, doc_ids, top_k=10
        )
        
        retrieved_ids = [doc_id for doc_id, _ in search_results]
        
        # Calculate metrics
        precision_at_1 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=1)
        precision_at_3 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=3)
        precision_at_5 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=5)
        recall_at_3 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=3)
        recall_at_5 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=5)
        mrr = RAGEvaluator.mean_reciprocal_rank(relevant_chunks, retrieved_ids)
        
        # Add to sums
        precision_at_1_sum += precision_at_1
        precision_at_3_sum += precision_at_3
        precision_at_5_sum += precision_at_5
        recall_at_3_sum += recall_at_3
        recall_at_5_sum += recall_at_5
        mrr_sum += mrr
    
    # Calculate averages
    num_queries = len(queries)
    metrics = {
        "precision_at_1": precision_at_1_sum / num_queries,
        "precision_at_3": precision_at_3_sum / num_queries,
        "precision_at_5": precision_at_5_sum / num_queries,
        "recall_at_3": recall_at_3_sum / num_queries,
        "recall_at_5": recall_at_5_sum / num_queries,
        "mrr": mrr_sum / num_queries
    }
    
    print(f"Metrics for original queries:")
    for name, value in metrics.items():
        print(f" {name}: {value:.4f}")
        
    return metrics

def evaluate_expanded_queries(
    queries: List[Dict[str, Any]],
    chunk_embeddings: np.ndarray,
    chunked_docs: List[Dict[str, str]],
    doc_ids: List[str],
    embedding_model: str,
    expansion_method: str = "simple",
    llm_function: Optional[Callable] = None
) -> Dict[str, float]:
    """Evaluate retrieval with query expansion"""
    # Initialize metrics
    precision_at_1_sum = 0
    precision_at_3_sum = 0
    precision_at_5_sum = 0
    recall_at_3_sum = 0
    recall_at_5_sum = 0
    mrr_sum = 0
    
    # Process each query
    for query_obj in tqdm(queries, desc=f"Expanded queries ({expansion_method})"):
        query = query_obj["question"]
        expected_answer = query_obj.get("answer", "")
        
        # Find relevant chunks (simplified approach)
        relevant_chunks = []
        
        query_words = set(query.lower().split())
        answer_words = set(expected_answer.lower().split())
        combined_words = query_words.union(answer_words)
        
        for i, doc_id in enumerate(doc_ids):
            # Get document text
            doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
            if doc_idx is None:
                continue
                
            doc_text = chunked_docs[doc_idx]["text"]
            doc_words = set(doc_text.lower().split())
            
            # Calculate word overlap
            overlap = len(combined_words.intersection(doc_words))
            if overlap >= min(3, len(combined_words)):
                relevant_chunks.append(doc_id)
        
        # If no relevant chunks found, use chunks with highest word overlap
        if not relevant_chunks:
            overlaps = []
            for i, doc_id in enumerate(doc_ids):
                doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
                if doc_idx is None:
                    continue
                    
                doc_text = chunked_docs[doc_idx]["text"]
                doc_words = set(doc_text.lower().split())
                overlap = len(combined_words.intersection(doc_words))
                overlaps.append((doc_id, overlap))
            
            # Sort by overlap and take top 2
            overlaps.sort(key=lambda x: x[1], reverse=True)
            relevant_chunks = [doc_id for doc_id, _ in overlaps[:2]]
        
        # Expand the query
        if expansion_method == "simple":
            expanded_queries = QueryProcessor.expand_query(query, method="simple")
        elif expansion_method == "llm" and llm_function:
            # In real implementation, have the LLM generate multiple query variants
            prompt = f"Generate 3 different versions of the following question that mean the same thing: '{query}'"
            llm_response = llm_function(prompt)
            
            # Parse response (assuming each variant is on a new line)
            expanded_queries = [query]  # Start with original
            for line in llm_response.strip().split('\n'):
                if line and not line.startswith("Generate"):
                    # Clean up line (remove numbers, quotes, etc.)
                    cleaned = line.strip('0123456789. "\'')
                    if cleaned:
                        expanded_queries.append(cleaned)
            
            # Ensure we have at least one query
            if len(expanded_queries) == 1:
                expanded_queries.append(f"Information about {query}")
                expanded_queries.append(f"Tell me about {query}")
        else:
            # Fallback to simple expansion
            expanded_queries = QueryProcessor.expand_query(query, method="simple")
        
        # Search with each expanded query
        all_results = []
        for expanded_query in expanded_queries:
            # Embed expanded query
            query_embedding = EmbeddingProvider.get_sentence_transformer_embeddings(
                [expanded_query], model_name=embedding_model
            )[0]
            
            # Search
            results = RetrievalMethods.vector_search(
                query_embedding, chunk_embeddings, doc_ids, top_k=5
            )
            
            all_results.extend(results)
        
        # Deduplicate while preserving order of first appearance
        seen = set()
        unique_results = []
        for doc_id, score in all_results:
            if doc_id not in seen:
                seen.add(doc_id)
                unique_results.append((doc_id, score))
        
        # Take top 10
        retrieved_ids = [doc_id for doc_id, _ in unique_results[:10]]
        
        # Calculate metrics
        precision_at_1 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=1)
        precision_at_3 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=3)
        precision_at_5 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=5)
        recall_at_3 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=3)
        recall_at_5 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=5)
        mrr = RAGEvaluator.mean_reciprocal_rank(relevant_chunks, retrieved_ids)
        
        # Add to sums
        precision_at_1_sum += precision_at_1
        precision_at_3_sum += precision_at_3
        precision_at_5_sum += precision_at_5
        recall_at_3_sum += recall_at_3
        recall_at_5_sum += recall_at_5
        mrr_sum += mrr
    
    # Calculate averages
    num_queries = len(queries)
    metrics = {
        "precision_at_1": precision_at_1_sum / num_queries,
        "precision_at_3": precision_at_3_sum / num_queries,
        "precision_at_5": precision_at_5_sum / num_queries,
        "recall_at_3": recall_at_3_sum / num_queries,
        "recall_at_5": recall_at_5_sum / num_queries,
        "mrr": mrr_sum / num_queries
    }
    
    print(f"Metrics for expanded queries ({expansion_method}):")
    for name, value in metrics.items():
        print(f" {name}: {value:.4f}")
        
    return metrics

def evaluate_hyde(
    queries: List[Dict[str, Any]],
    chunk_embeddings: np.ndarray,
    chunked_docs: List[Dict[str, str]],
    doc_ids: List[str],
    embedding_model: str,
    llm_function: Callable
) -> Dict[str, float]:
    """Evaluate retrieval with HyDE (Hypothetical Document Embeddings)"""
    # Initialize metrics
    precision_at_1_sum = 0
    precision_at_3_sum = 0
    precision_at_5_sum = 0
    recall_at_3_sum = 0
    recall_at_5_sum = 0
    mrr_sum = 0
    
    # Process each query
    for query_obj in tqdm(queries, desc="HyDE"):
        query = query_obj["question"]
        expected_answer = query_obj.get("answer", "")
        
        # Find relevant chunks (simplified approach)
        relevant_chunks = []
        
        query_words = set(query.lower().split())
        answer_words = set(expected_answer.lower().split())
        combined_words = query_words.union(answer_words)
        
        for i, doc_id in enumerate(doc_ids):
            # Get document text
            doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
            if doc_idx is None:
                continue
                
            doc_text = chunked_docs[doc_idx]["text"]
            doc_words = set(doc_text.lower().split())
            
            # Calculate word overlap
            overlap = len(combined_words.intersection(doc_words))
            if overlap >= min(3, len(combined_words)):
                relevant_chunks.append(doc_id)
        
        # If no relevant chunks found, use chunks with highest word overlap
        if not relevant_chunks:
            overlaps = []
            for i, doc_id in enumerate(doc_ids):
                doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
                if doc_idx is None:
                    continue
                    
                doc_text = chunked_docs[doc_idx]["text"]
                doc_words = set(doc_text.lower().split())
                overlap = len(combined_words.intersection(doc_words))
                overlaps.append((doc_id, overlap))
            
            # Sort by overlap and take top 2
            overlaps.sort(key=lambda x: x[1], reverse=True)
            relevant_chunks = [doc_id for doc_id, _ in overlaps[:2]]
        
        # Apply HyDE
        _, hypothetical_doc = QueryProcessor.hyde_expansion(query, llm_function)
        
        # Embed the hypothetical document instead of query
        doc_embedding = EmbeddingProvider.get_sentence_transformer_embeddings(
            [hypothetical_doc], model_name=embedding_model
        )[0]
        
        # Search using the document embedding
        search_results = RetrievalMethods.vector_search(
            doc_embedding, chunk_embeddings, doc_ids, top_k=10
        )
        
        retrieved_ids = [doc_id for doc_id, _ in search_results]
        
        # Calculate metrics
        precision_at_1 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=1)
        precision_at_3 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=3)
        precision_at_5 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=5)
        recall_at_3 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=3)
        recall_at_5 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=5)
        mrr = RAGEvaluator.mean_reciprocal_rank(relevant_chunks, retrieved_ids)
        
        # Add to sums
        precision_at_1_sum += precision_at_1
        precision_at_3_sum += precision_at_3
        precision_at_5_sum += precision_at_5
        recall_at_3_sum += recall_at_3
        recall_at_5_sum += recall_at_5
        mrr_sum += mrr
    
    # Calculate averages
    num_queries = len(queries)
    metrics = {
        "precision_at_1": precision_at_1_sum / num_queries,
        "precision_at_3": precision_at_3_sum / num_queries,
        "precision_at_5": precision_at_5_sum / num_queries,
        "recall_at_3": recall_at_3_sum / num_queries,
        "recall_at_5": recall_at_5_sum / num_queries,
        "mrr": mrr_sum / num_queries
    }
    
    print(f"Metrics for HyDE:")
    for name, value in metrics.items():
        print(f" {name}: {value:.4f}")
        
    return metrics

def evaluate_enhanced_hyde(
    queries: List[Dict[str, Any]],
    chunk_embeddings: np.ndarray,
    chunked_docs: List[Dict[str, str]],
    doc_ids: List[str],
    embedding_model: str,
    llm_function: Callable
) -> Dict[str, float]:
    """Evaluate retrieval with Enhanced HyDE"""
    # Initialize metrics
    precision_at_1_sum = 0
    precision_at_3_sum = 0
    precision_at_5_sum = 0
    recall_at_3_sum = 0
    recall_at_5_sum = 0
    mrr_sum = 0
    
    # Create Enhanced HyDE processor
    hyde = EnhancedHyDE(embedding_model)
    
    # Process each query
    for query_obj in tqdm(queries, desc="Enhanced HyDE"):
        query = query_obj["question"]
        expected_answer = query_obj.get("answer", "")
        
        # Find relevant chunks (simplified approach) - same as in other evaluation functions
        relevant_chunks = []
        query_words = set(query.lower().split())
        answer_words = set(expected_answer.lower().split())
        combined_words = query_words.union(answer_words)
        
        for i, doc_id in enumerate(doc_ids):
            doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
            if doc_idx is None:
                continue
                
            doc_text = chunked_docs[doc_idx]["text"]
            doc_words = set(doc_text.lower().split())
            
            # Calculate word overlap
            overlap = len(combined_words.intersection(doc_words))
            if overlap >= min(3, len(combined_words)):
                relevant_chunks.append(doc_id)
                
        # If no relevant chunks found, use chunks with highest word overlap
        if not relevant_chunks:
            overlaps = []
            for i, doc_id in enumerate(doc_ids):
                doc_idx = next((j for j, doc in enumerate(chunked_docs) if doc["chunk_id"] == doc_id), None)
                if doc_idx is None:
                    continue
                    
                doc_text = chunked_docs[doc_idx]["text"]
                doc_words = set(doc_text.lower().split())
                
                overlap = len(combined_words.intersection(doc_words))
                overlaps.append((doc_id, overlap))
                
            # Sort by overlap and take top 2
            overlaps.sort(key=lambda x: x[1], reverse=True)
            relevant_chunks = [doc_id for doc_id, _ in overlaps[:2]]
            
        # Retrieve using Enhanced HyDE
        results, hypothetical_docs = hyde.hyde_retrieval(
            query, 
            chunk_embeddings, 
            doc_ids, 
            llm_function, 
            top_k=10,
            multi_document=False
        )
        
        # Extract retrieved document IDs
        retrieved_ids = [doc_id for doc_id, _ in results]
        
        # Calculate metrics
        precision_at_1 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=1)
        precision_at_3 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=3)
        precision_at_5 = RAGEvaluator.precision_at_k(relevant_chunks, retrieved_ids, k=5)
        recall_at_3 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=3)
        recall_at_5 = RAGEvaluator.recall_at_k(relevant_chunks, retrieved_ids, k=5)
        mrr = RAGEvaluator.mean_reciprocal_rank(relevant_chunks, retrieved_ids)
        
        # Add to sums
        precision_at_1_sum += precision_at_1
        precision_at_3_sum += precision_at_3
        precision_at_5_sum += precision_at_5
        recall_at_3_sum += recall_at_3
        recall_at_5_sum += recall_at_5
        mrr_sum += mrr
        
    # Calculate averages
    num_queries = len(queries)
    metrics = {
        "precision_at_1": precision_at_1_sum / num_queries,
        "precision_at_3": precision_at_3_sum / num_queries,
        "precision_at_5": precision_at_5_sum / num_queries,
        "recall_at_3": recall_at_3_sum / num_queries,
        "recall_at_5": recall_at_5_sum / num_queries,
        "mrr": mrr_sum / num_queries
    }
    
    print(f"Metrics for Enhanced HyDE:")
    for name, value in metrics.items():
        print(f" {name}: {value:.4f}")
        
    return metrics

def load_best_configuration(results_dir: str) -> Dict[str, Any]:
    """
    Load the best configuration from previous experiments
    
    Args:
        results_dir: Directory with experiment results
        
    Returns:
        Dictionary with best chunking and embedding settings
    """
    best_config = {}
    
    # Try to find previous experiment results
    if not os.path.exists(results_dir):
        return best_config
        
    # Look for chunking experiment results
    chunking_dirs = [d for d in os.listdir(results_dir) 
                    if d.startswith("chunking_experiment") and os.path.isdir(os.path.join(results_dir, d))]
                    
    if chunking_dirs:
        latest_chunking = max(chunking_dirs)
        chunking_results = os.path.join(results_dir, latest_chunking, "results.csv")
        
        if os.path.exists(chunking_results):
            # Load results and find best configuration by MRR
            try:
                df = pd.read_csv(chunking_results)
                
                if "metric_mrr" in df.columns:
                    best_idx = df["metric_mrr"].idxmax()
                    best_row = df.loc[best_idx]
                    
                    best_config["chunking_strategy"] = best_row.get("chunking_strategy")
                    
                    if best_row.get("chunking_strategy") == "fixed":
                        best_config["chunk_size"] = best_row.get("chunk_size")
                        best_config["chunk_overlap"] = best_row.get("chunk_overlap")
            except:
                pass
                
    # Look for embedding experiment results
    embedding_dirs = [d for d in os.listdir(results_dir) 
                     if d.startswith("embedding_experiment") and os.path.isdir(os.path.join(results_dir, d))]
                     
    if embedding_dirs:
        latest_embedding = max(embedding_dirs)
        embedding_results = os.path.join(results_dir, latest_embedding, "results.csv")
        
        if os.path.exists(embedding_results):
            # Load results and find best configuration by MRR
            try:
                df = pd.read_csv(embedding_results)
                
                if "metric_mrr" in df.columns:
                    best_idx = df["metric_mrr"].idxmax()
                    best_row = df.loc[best_idx]
                    
                    best_config["embedding_model"] = best_row.get("embedding_model")
            except:
                pass
                
    return best_config

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Run query processing experiments")
    parser.add_argument("--corpus", type=str, default="data/sample_corpus.pkl", help="Path to corpus file")
    parser.add_argument("--queries", type=str, default="data/sample_queries.pkl", help="Path to queries file")
    parser.add_argument("--sample-size", type=int, default=50, help="Number of documents to use")
    parser.add_argument("--num-queries", type=int, default=5, help="Number of queries to evaluate")
    parser.add_argument("--chunk-strategy", type=str, default="fixed", help="Chunking strategy to use")
    parser.add_argument("--chunk-size", type=int, default=128, help="Size of chunks for fixed strategy")
    parser.add_argument("--chunk-overlap", type=int, default=0, help="Overlap for fixed chunking")
    parser.add_argument("--embedding-model", type=str, default="all-MiniLM-L6-v2", help="Embedding model to use")
    parser.add_argument("--output-dir", type=str, default="results", help="Directory to save results")
    
    args = parser.parse_args()
    
    run_query_processing_experiment(
        corpus_file=args.corpus,
        queries_file=args.queries,
        sample_size=args.sample_size,
        num_queries=args.num_queries,
        chunk_strategy=args.chunk_strategy,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        embedding_model=args.embedding_model,
        output_dir=args.output_dir
    )