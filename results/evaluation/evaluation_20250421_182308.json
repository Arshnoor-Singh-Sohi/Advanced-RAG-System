[
  {
    "config_name": "default",
    "config": {
      "corpus_path": "data\\corpus.pkl",
      "index_path": "data/faiss_index",
      "chunking_strategy": "recursive",
      "chunk_size": 1000,
      "chunk_overlap": 200,
      "embedding_model": "all-MiniLM-L6-v2",
      "retrieval_method": "hybrid",
      "retrieval_alpha": 0.7,
      "top_k": 7,
      "query_expansion": false,
      "expansion_method": "simple",
      "use_hyde": true,
      "hyde_method": "basic",
      "hyde_num_docs": 3,
      "use_reranking": true,
      "reranking_method": "multi_stage",
      "reranking_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
      "reranking_stages": [
        "semantic",
        "cross_encoder",
        "diversity"
      ],
      "diversity_alpha": 0.7,
      "prompt_template": "Answer the question based ONLY on the following context:\n\nContext: {context}\n\nQuestion: {query}\n\nAnswer:",
      "temperature": 0.5,
      "use_mmr": false,
      "mmr_fetch_k": 20,
      "mmr_lambda": 0.5,
      "bm25_k1": 1.5,
      "bm25_b": 0.75,
      "query_transformation": "none",
      "multiquery_num_variations": 3
    },
    "timestamp": "2025-04-21T18:23:08.806244",
    "dataset_name": "rag_example",
    "question_results": [
      {
        "question": "What is Retrieval-Augmented Generation (RAG)?",
        "expected_answer": "Retrieval-Augmented Generation (RAG) is a technique that combines the power of large language models with information retrieval systems. It enhances the quality of generated text by retrieving relevant information from a knowledge base.",
        "generated_answer": "I don't have enough information to provide details on Retrieval-Augmented Generation (RAG) based on the context provided.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.4093878984451294,
        "answer_time": 0.6040233850479125,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.22641509433962262,
        "faithfulness": 0.0,
        "category": "definition",
        "difficulty": "easy"
      },
      {
        "question": "What are the main components of a RAG system?",
        "expected_answer": "A RAG system typically consists of three main components: 1) A Retriever that finds relevant information, 2) A Generator that creates responses, and 3) A Knowledge Base of documents or facts.",
        "generated_answer": "I don't have enough information to provide an answer regarding the main components of a RAG system based on the context provided.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.168901753425598,
        "answer_time": 0.5009578943252563,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.25925925925925924,
        "faithfulness": 0.0,
        "category": "components",
        "difficulty": "easy"
      },
      {
        "question": "How does vector search work in RAG systems?",
        "expected_answer": "Vector search in RAG systems works by converting text into numerical vectors and finding documents with similar vector representations. This allows for semantic searching beyond simple keyword matching.",
        "generated_answer": "I don't have enough information to provide an answer on how vector search works in RAG systems based on the given context.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.2312614917755127,
        "answer_time": 0.5276834964752197,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.23529411764705882,
        "faithfulness": 0.0,
        "category": "technique",
        "difficulty": "medium"
      },
      {
        "question": "What are embedding models and why are they important for RAG?",
        "expected_answer": "Embedding models transform text into numerical vectors that capture semantic meaning. They are crucial for RAG systems because they enable semantic similarity search, allowing the system to find relevant documents even when the wording differs from the query.",
        "generated_answer": "I don't have enough information to provide an answer to the question about embedding models and their importance for RAG based on the given context.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.2697054624557493,
        "answer_time": 0.5441594839096069,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.21875,
        "faithfulness": 0.0,
        "category": "technique",
        "difficulty": "medium"
      },
      {
        "question": "What is document chunking and why is it important in RAG systems?",
        "expected_answer": "Document chunking is the process of splitting documents into smaller pieces to enable more precise retrieval. It's important for RAG systems because it allows the retriever to find specific relevant sections rather than entire documents, improving the precision of the generated answers.",
        "generated_answer": "I don't have enough information to provide an answer to the question about document chunking and its importance in RAG systems based on the given context.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.3864749908447265,
        "answer_time": 0.5942035675048828,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.22857142857142856,
        "faithfulness": 0.0,
        "category": "technique",
        "difficulty": "medium"
      },
      {
        "question": "What is the difference between BM25 and vector search in RAG systems?",
        "expected_answer": "BM25 is a keyword-based search algorithm that ranks documents based on term frequency and document length, while vector search uses semantic embeddings to find documents with similar meaning regardless of specific keywords. BM25 is better at exact matches, while vector search excels at understanding concepts and semantics.",
        "generated_answer": "I don't have enough information to provide a comprehensive answer on the difference between BM25 and vector search in RAG systems based on the context provided.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.2320066690444946,
        "answer_time": 0.5280028581619263,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.21333333333333335,
        "faithfulness": 0.0,
        "category": "comparison",
        "difficulty": "hard"
      },
      {
        "question": "What is reranking in the context of RAG, and why is it useful?",
        "expected_answer": "Reranking in RAG is the process of taking initially retrieved documents and reordering them using more sophisticated (often computationally expensive) models or criteria. It's useful because it improves precision by applying more advanced relevance judgments after the initial retrieval step, which helps provide the generator with higher quality context.",
        "generated_answer": "I don't have enough information to provide an answer regarding reranking in the context of RAG.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.2382339477539062,
        "answer_time": 0.5306716918945312,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.208955223880597,
        "faithfulness": 0.0,
        "category": "technique",
        "difficulty": "hard"
      },
      {
        "question": "How does the chunking strategy affect RAG performance?",
        "expected_answer": "The chunking strategy affects RAG performance by determining how documents are split into retrievable pieces. Different strategies (fixed-size, sentence-based, paragraph-based, semantic) influence whether related information stays together or gets split apart. This impacts retrieval precision, as overly large chunks might contain irrelevant information, while too small chunks might lose context.",
        "generated_answer": "I don't have enough information to provide a comprehensive answer on how the chunking strategy affects RAG performance based on the given context.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.5453835010528563,
        "answer_time": 0.6623072147369384,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.25974025974025977,
        "faithfulness": 0.0,
        "category": "performance",
        "difficulty": "hard"
      },
      {
        "question": "What is Maximal Marginal Relevance (MMR) in RAG systems?",
        "expected_answer": "Maximal Marginal Relevance (MMR) is a technique used in RAG systems to balance relevance and diversity in retrieved documents. It works by selecting documents that are both relevant to the query and different from already selected documents, which helps reduce redundancy in the context provided to the generator.",
        "generated_answer": "I don't have enough information to provide an answer regarding Maximal Marginal Relevance (MMR) in RAG systems based on the given context.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.4860219955444336,
        "answer_time": 0.636866569519043,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.28169014084507044,
        "faithfulness": 0.0,
        "category": "technique",
        "difficulty": "hard"
      },
      {
        "question": "What is HyDE (Hypothetical Document Embeddings) in RAG?",
        "expected_answer": "HyDE (Hypothetical Document Embeddings) is a technique where the system first generates a hypothetical document that might contain the answer to a query, then uses the embedding of this document for retrieval instead of the query embedding. This bridges the gap between questions and answers in vector space, improving retrieval quality especially for complex queries.",
        "generated_answer": "I don't have enough information to provide details on HyDE (Hypothetical Document Embeddings) in RAG based on the context provided.",
        "retrieved_docs": [
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown",
          "Source Unknown"
        ],
        "relevant_docs": [],
        "retrieval_time": 1.2305834054946898,
        "answer_time": 0.5273928880691529,
        "precision": 0.0,
        "recall": 0.0,
        "mrr": 0.0,
        "rouge1": 0.18421052631578946,
        "faithfulness": 0.0,
        "category": "technique",
        "difficulty": "hard"
      }
    ],
    "aggregate_metrics": {
      "avg_retrieval_time": 1.3197961115837096,
      "avg_answer_time": 0.565626904964447,
      "avg_precision": 0.0,
      "avg_recall": 0.0,
      "avg_mrr": 0.0,
      "avg_rouge1": 0.23162193839324194,
      "avg_faithfulness": 0.0,
      "question_count": 10
    }
  }
]